## âœ… Refined Requirements (So Far)

### ğŸ“± Platforms

- **Android**: Primary mobile experience
    
- **Windows**: Optional desktop version (likely easier to prototype and debug)
    

### ğŸ“‚ Input Formats

- `.txt`, `.html`, `.epub`
    
- Future: `.docx`, `.md`, `.pdf`, `.mobi`
    

### ğŸ§¹ Preprocessing (Planned)

- Remove:
    
    - Hard line breaks
        
    - Page numbers
        
    - Headers/footers
        
    - Special symbols (e.g., OCR artifacts)
        
- Normalize:
    
    - Paragraphs
        
    - Quotation marks
        
    - Unicode cleanup
        

### ğŸ›ï¸ Controls (MVP)

- Play / Pause
    
- File select
    
- Auto-resume from last position
    

### ğŸ§  AI Voice & Comprehension

- **Goal**: Use Copilot-quality voice and understanding, not Android's default TTS
    
- **Future**: Voice interaction like:
    
    - â€œWhat does â€˜diffidentâ€™ mean?â€
        
    - â€œSummarize this chapterâ€
        
    - â€œWho is the main character?â€
        

## ğŸ”Š How to Harness AI Voice + Comprehension

### 1. **Voice Engine Options**

To replicate Copilotâ€™s natural voice quality, youâ€™ll want **cloud-based neural TTS**:

|Provider|Pros|Notes|
|---|---|---|
|**Microsoft Azure TTS**|Same tech as Copilot, natural voices|Best match|
|Google Cloud TTS|Widely used, multilingual|Slightly robotic at times|
|Amazon Polly|Good quality, SSML support|Less expressive|

**Recommendation**: Start with **Azure TTS**. It supports:

- Natural-sounding neural voices
    
- SSML for emphasis, pauses, etc.
    
- Streaming audio (for real-time playback)
    

### 2. **Architecture for AI Voice**

- **App** sends cleaned text to Azure TTS API
    
- **Receives** audio stream or file
    
- **Plays** audio via media player
    
- **Caches** audio for offline replay (optional)
    

### 3. **Comprehension Layer (Future)**

To support interactions like â€œWhat does this mean?â€:

- Use **Copilot-style LLM backend** (e.g., Azure OpenAI or local LLM)
    
- Maintain **context window** of current paragraph/chapter
    
- On voice command:
    
    - Transcribe voice (using Whisper or Android SpeechRecognizer)
        
    - Parse intent (e.g., define, summarize, explain)
        
    - Query LLM with context + question
        
    - Respond via TTS
        

## ğŸ§  Example Flow: â€œWhat does â€˜diffidentâ€™ mean?â€

1. User says: â€œWhat does diffident mean?â€
    
2. App transcribes voice
    
3. App sends: `Define 'diffident' in this context: [current paragraph]` to LLM
    
4. LLM replies: â€œIt means shy or lacking confidence.â€
    
5. App reads response aloud using Azure TTS
    

## ğŸ§ª Next Steps

1. **Prototype Windows version** (easier for debugging, file access, and TTS testing)
    
2. **Integrate Azure TTS** for natural voice
    
3. **Build text cleaner module** (regex + heuristics)
    
4. **Design minimal UI**: file picker, play/pause, progress bar
    
5. **Plan for LLM integration** (Copilot-style comprehension)
    
## Diagram

+---------------------+
|  User Interface     | â† Windows (WPF/WinUI) or Android (Jetpack Compose)
+---------------------+
          â†“
+---------------------+
|  File Loader        | â† Reads .txt, .html, .epub, .pdf, .mobi
+---------------------+
          â†“
+---------------------+
|  Text Cleaner       | â† Removes line breaks, page numbers, symbols
+---------------------+
          â†“
+---------------------+
|  TTS Engine         | â† Azure TTS API (neural voice)
+---------------------+
          â†“
+---------------------+
|  Audio Player       | â† Streams or plays cached audio
+---------------------+
          â†“
+---------------------+
|  Progress Tracker   | â† Saves last position per book
+---------------------+
